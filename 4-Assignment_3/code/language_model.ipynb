{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a040635-8925-4a85-8122-f48dea847a78",
   "metadata": {},
   "source": [
    "<style>\n",
    "    h1, h2, h3, h4, h5, h6 { \n",
    "        margin-bottom: 0.5em; \n",
    "        margin-top: 0.5em; \n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575090c9-651b-46a2-aa17-fa3f67880423",
   "metadata": {},
   "source": [
    "<span style=\"line-height: 0.5;\"><h1><center>Deep Learning - PhD Course</center></h1></span>\n",
    "<span style=\"line-height: 0.5;\"><h2><center>Hand-in assignment 3</center></h2></span>\n",
    "<span style=\"line-height: 0.5;\"><h3><center>Usama Zafar</center></h3></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdf1c08-b65e-404f-9efe-b74e77db3cf9",
   "metadata": {},
   "source": [
    "## Part 1: Dataset Loading and Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52602d93-0632-4515-b162-d1ff2c1c084b",
   "metadata": {},
   "source": [
    "### 1. Read and print out some of the PTB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4fe659-6a69-48e7-a875-8a87d7718eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Line 1:  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
      "Train Data Line 2:  pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
      "Train Data Line 3:  mr. <unk> is chairman of <unk> n.v. the dutch publishing group \n",
      "\n",
      "Valid Data Line 1:  consumers may want to move their telephones a little closer to the tv set \n",
      "Valid Data Line 2:  <unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play in N years from among four or five <unk> <unk> \n",
      "Valid Data Line 3:  two weeks ago viewers of several nbc <unk> consumer segments started calling a N number for advice on various <unk> issues \n",
      "\n",
      "Test Data Line 1:  no it was n't black monday \n",
      "Test Data Line 2:  but while the new york stock exchange did n't fall apart friday as the dow jones industrial average plunged N points most of it in the final hour it barely managed to stay this side of chaos \n",
      "Test Data Line 3:  some circuit breakers installed after the october N crash failed their first test traders say unable to cool the selling panic in both stocks and futures \n"
     ]
    }
   ],
   "source": [
    "# setup path for the dataset\n",
    "dataset_path = \"./dataset/data/ptb.{}.txt\"\n",
    "\n",
    "# open the text file containing train data and print out couple of lines\n",
    "with open(dataset_path.format(\"train\"), 'r') as f:\n",
    "    print(f\"Train Data Line 1: {f.readline()}\", end=\"\")\n",
    "    print(f\"Train Data Line 2: {f.readline()}\", end=\"\")\n",
    "    print(f\"Train Data Line 3: {f.readline()}\")\n",
    "\n",
    "# open the text file containing valid data and print out couple of lines\n",
    "with open(dataset_path.format(\"valid\"), 'r') as f:\n",
    "    print(f\"Valid Data Line 1: {f.readline()}\", end=\"\")\n",
    "    print(f\"Valid Data Line 2: {f.readline()}\", end=\"\")\n",
    "    print(f\"Valid Data Line 3: {f.readline()}\")\n",
    "    \n",
    "# open the text file containing test data and print out couple of lines\n",
    "with open(dataset_path.format(\"test\"), 'r') as f:\n",
    "    print(f\"Test Data Line 1: {f.readline()}\", end=\"\")\n",
    "    print(f\"Test Data Line 2: {f.readline()}\", end=\"\")\n",
    "    print(f\"Test Data Line 3: {f.readline()}\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fbac5e-2eab-4bf8-a931-872bb1732f20",
   "metadata": {},
   "source": [
    "### 2. Load the entire PTB dataset while modifying end of line \"\\n\" with \"&lt;eos&gt;\" tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ba184c-e095-4cc1-828b-3bd1d2abafb4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter <eos> pierre <unk> N years old will join the board as a nonexecutive director nov. N <eos> mr. <unk> is chairman of <unk> n.\n",
      "\n",
      "Valid Data:\n",
      " consumers may want to move their telephones a little closer to the tv set <eos> <unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play in N years from among four or five <unk> <unk> <eos> two weeks ago viewers of several nbc <unk> consumer segments started\n",
      "\n",
      "Test Data:\n",
      " no it was n't black monday <eos> but while the new york stock exchange did n't fall apart friday as the dow jones industrial average plunged N points most of it in the final hour it barely managed to stay this side of chaos <eos> some circuit breakers installed after the october N crash failed thei\n"
     ]
    }
   ],
   "source": [
    "train_data = None\n",
    "valid_data = None\n",
    "test_data = None\n",
    "\n",
    "# open the text file containing train data and print out couple of lines\n",
    "with open(dataset_path.format(\"train\"), 'r') as f:\n",
    "    train_data = f.read().replace(\"\\n\", \"<eos>\")\n",
    "    #train_data = [sentence.replace(\"\\n\", \"<eos>\") for sentence in f.readlines()]\n",
    "    print(\"Train Data:\")\n",
    "    print(train_data[:300])\n",
    "        \n",
    "\n",
    "# open the text file containing valid data and print out couple of lines\n",
    "with open(dataset_path.format(\"valid\"), 'r') as f:\n",
    "    valid_data = f.read().replace(\"\\n\", \"<eos>\")\n",
    "    #valid_data = [sentence.replace(\"\\n\", \"<eos>\") for sentence in f.readlines()]\n",
    "    print(\"\\nValid Data:\")\n",
    "    print(valid_data[:300])\n",
    "    \n",
    "# open the text file containing test data and print out couple of lines\n",
    "with open(dataset_path.format(\"test\"), 'r') as f:\n",
    "    test_data = f.read().replace(\"\\n\", \"<eos>\")\n",
    "    #test_data = [sentence.replace(\"\\n\", \"<eos>\") for sentence in f.readlines()]\n",
    "    print(\"\\nTest Data:\")\n",
    "    print(test_data[:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89744114-8415-403d-bb9d-25d45b04373c",
   "metadata": {},
   "source": [
    "### 3. Count total number of words in each of PTB dataset splits [Train, Valid, Test]:\n",
    "1. First we build list of words in each of the splits\n",
    "1. Next we count number of words per split and display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1798d5bd-1035-4e4a-a6fc-797e94dfc7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD COUNT: Train = 929590, Valid = 73761, Test = 82431\n"
     ]
    }
   ],
   "source": [
    "train_split_words = train_data.split(\" \")\n",
    "valid_split_words = valid_data.split(\" \")\n",
    "test_split_words = test_data.split(\" \")\n",
    "\n",
    "print(f\"WORD COUNT: Train = {len(train_split_words)}, Valid = {len(valid_split_words)}, Test = {len(test_split_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32fd572-7b76-4b07-96ef-c99ff29c6f89",
   "metadata": {},
   "source": [
    "### 4. Count unique words in all the splits combined as well as generated a dictionary for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "668414d0-2da8-4d1a-affa-5d6bf4b8fe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words: ['', 'aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the']\n"
     ]
    }
   ],
   "source": [
    "# first combine all words obtained form all splits into a single list\n",
    "all_words = []\n",
    "all_words.extend(train_split_words)\n",
    "all_words.extend(valid_split_words)\n",
    "all_words.extend(test_split_words)\n",
    "\n",
    "# print out some samples for verification\n",
    "print(f\"Total Words: {all_words[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8224d57-8e9a-4565-9730-33a350742827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words = 10001\n"
     ]
    }
   ],
   "source": [
    "# convert the list of words into a set\n",
    "# this will yield unique words which we\n",
    "# can use to create our index dictionary\n",
    "unique_words = set(all_words)\n",
    "\n",
    "# print out number of unique words\n",
    "print(f\"Number of unique words = {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c09d49-0031-4174-a62e-dfd2fa74ef4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words = 10001\n"
     ]
    }
   ],
   "source": [
    "# now let's create a dictionary by assigning index\n",
    "# to each unique word that appears in our dataset\n",
    "word_index_dict = {k: v for v, k in enumerate(unique_words)}\n",
    "\n",
    "# print some of the values from the newly created\n",
    "# dictionary for cross verification\n",
    "print(f\"Number of unique words = {len(word_index_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547cbf39-f536-424b-9306-138ced7dd0cf",
   "metadata": {},
   "source": [
    "### 5. Create a function that spits integer sequence given text and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6799ac8-62b4-48c6-85e7-e9d2e4c0fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function that accepts a sentence \n",
    "# and a dictionary as input to generate integer\n",
    "# encoding for the provided sentence\n",
    "def integer_encode_sentence(input_sentence: str, word_dictionary: dict):\n",
    "    return [word_dictionary[word] for word in input_sentence.split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "006aa62c-28d5-4174-a105-62cea4bc1308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: \n",
      " aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter <eos> pierre <unk>\n",
      "\n",
      " Integer Encoding: \n",
      "[0, 2882, 8241, 7628, 5539, 3068, 6994, 6782, 5324, 3215, 1942, 8179, 984, 2898, 5466, 6554, 4754, 2178, 5758, 6523, 222, 8043, 2337, 6840, 9809, 359, 1613, 3136]\n"
     ]
    }
   ],
   "source": [
    "# let test the above function some random \n",
    "# sentences from the PTB dataset\n",
    "print(f\"Original Text: \\n{train_data[:194]}\\n\\n Integer Encoding: \\n{integer_encode_sentence(train_data[:194],word_index_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6fa0beb-b04b-4b20-afb2-89fe9a7b5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import open\n",
    "import torch\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'ptb.train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'ptb.valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'ptb.test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            idss = []\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                ids = []\n",
    "                for word in words:\n",
    "                    ids.append(self.dictionary.word2idx[word])\n",
    "                idss.append(torch.tensor(ids).type(torch.int64))\n",
    "            ids = torch.cat(idss)\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d35ef7b-e5dc-44df-8c80-54c74d7ba640",
   "metadata": {},
   "source": [
    "## Part 2: Recurrent Neural Network [RNN] - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e502181-211e-4364-8d4d-59a17c59cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10fe271-23f7-48e8-9a93-42c09b9107e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check which device to run the code on\n",
    "# use GPU whenever it is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d349791-a70e-42e0-b78e-8b3f0beb86dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and setup hyper-parameters for the model training\n",
    "batch_size = 20\n",
    "sequence_length = 10\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea4242f-9f83-494a-b10f-605b581c09bb",
   "metadata": {},
   "source": [
    "### Create a function that converts our raw data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b113b561-eb5f-45a4-9c58-516b0ccadc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formulate batches based on given batch size\n",
    "def create_batches(in_data: torch.tensor, batch_size: int):\n",
    "    # find number of batches\n",
    "    print(in_data.size(0))\n",
    "    n_batches = in_data.size(0) // batch_size\n",
    "    # clean data by dropping last few words\n",
    "    # that won't fit the batch_size (trimming)\n",
    "    in_data = in_data.narrow(0, 0, n_batches * batch_size)\n",
    "    # finaly create batches of the data\n",
    "    in_data = in_data.view(batch_size, -1).t().contiguous()\n",
    "    # return the finalized data\n",
    "    return in_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c281646-8d42-4661-b9b5-60510122dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the entire train, valid, and test set into integers\n",
    "#encoded_train = torch.tensor(integer_encode_sentence(train_data,word_index_dict)).type(torch.int64)\n",
    "#encoded_valid = torch.tensor(integer_encode_sentence(valid_data,word_index_dict)).type(torch.int64)\n",
    "#encoded_test = torch.tensor(integer_encode_sentence(test_data,word_index_dict)).type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6da54c-40b2-46fb-ba69-c194ee0a5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(\"./dataset/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9730ff4e-117e-45ab-9151-1242a857722d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus.dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c85f214-0902-4f61-926e-5d22c0c094c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929589\n",
      "73760\n",
      "82430\n"
     ]
    }
   ],
   "source": [
    "# test the above routine and create batches\n",
    "train_data = create_batches(corpus.train, batch_size=batch_size)\n",
    "valid_data = create_batches(corpus.valid, batch_size=batch_size)\n",
    "test_data = create_batches(corpus.test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "810039ac-80cf-4f20-8d29-8cf12b118b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46479, 20])\n"
     ]
    }
   ],
   "source": [
    "# test cell to check if data has been processed properly or not\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e94a7-450f-498c-a3b3-ef5a8e1262fb",
   "metadata": {},
   "source": [
    "### Define our RNN model's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "970aed84-46b7-4c39-8fe4-f2e7de371bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple RNN model\n",
    "class simpleRNN(nn.Module):\n",
    "    def __init__(self, num_tokens, embedding_dim, hidden_dim, num_layers, dropout=0.5):\n",
    "        super(simpleRNN, self).__init__()\n",
    "        # save parameters as class variables\n",
    "        self.num_tokens, self.embedding_dim, self.hidden_dim, self.num_layers = num_tokens, embedding_dim, hidden_dim, num_layers\n",
    "        # create an embedding layer\n",
    "        self.embedding_layers = nn.Embedding(num_tokens, embedding_dim)\n",
    "        # create hidden RNN layers\n",
    "        #self.recurrent_layers = nn.RNN( input_size=embedding_dim, hidden_size=hidden_dim, dropout=dropout, num_layers=num_layers, batch_first=True)\n",
    "        self.recurrent_layers = nn.RNN(embedding_dim, hidden_dim, num_layers, dropout=dropout)#, batch_first=True)\n",
    "        # create output layer\n",
    "        #self.output_layers = nn.Linear(in_features=hidden_dim, out_features=num_tokens)\n",
    "        self.output_layers = nn.Linear(hidden_dim, num_tokens)\n",
    "        # initialize weights\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # apply embedding layer first\n",
    "        embedded = self.embedding_layers(x)\n",
    "        # apply the hidden RNN layers next\n",
    "        output, hidden = self.recurrent_layers(embedded, hidden)\n",
    "        # run the output through  \n",
    "        # the final output layers\n",
    "        output = self.output_layers(output)\n",
    "        # apply the softmax function on output\n",
    "        output = output.view(-1, self.num_tokens)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        # return the output generated\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.embedding_layers.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.output_layers.bias)\n",
    "        nn.init.uniform_(self.output_layers.weight, -initrange, initrange)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters())\n",
    "        return weight.new_zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "        #return (weight.new_zeros(self.num_layers, batch_size, self.hidden_dim), weight.new_zeros(self.num_layers, batch_size, self.hidden_dim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86b1d2-4339-4793-adcd-77f51fb1be01",
   "metadata": {},
   "source": [
    "### Create a function to fetch a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0656f58-249a-4c1a-9fb8-9370678b295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a batching routine to get next data batch\n",
    "def get_batch(data_source, seq_length, start_index):\n",
    "    current_length = min(seq_length, len(data_source) - 1 - start_index)\n",
    "    data = data_source[start_index:start_index+current_length]\n",
    "    target = data_source[start_index+1:start_index+current_length+1].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fbb0f7d-5dd5-4f0b-bb19-4ee826a85a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  10,   78, 4255, 2745, 4381, 4004, 3568, 9290, 1771,   78,  727, 2459,\n",
      "           26, 1051,   48,   54,  374, 1061,   27,   32],\n",
      "        [  11,  718,  119,   64, 4778,   26,  753,   48,  668,   69, 2696,  531,\n",
      "          553,   35, 1843,  417,  131,   57,  187,   26],\n",
      "        [  12,   26,   27,  979,   35, 1656,   40,   39,  160,  280,   42,   32,\n",
      "           39,   26,  823,  213,   26, 3687, 2833, 1486],\n",
      "        [  13, 2966,  930, 4273,   60,   26, 4135, 8547,   35,   35,  314,  310,\n",
      "          310, 4816, 2058, 6019, 6384,  718,  416, 9054],\n",
      "        [  14,  467, 1652, 3817,   42, 3320, 4897, 3672,  290,  852,   48, 9723,\n",
      "           69, 3126,   24,   93,   24,  220,   27,   24]]) \n",
      "\n",
      " tensor([  11,  718,  119,   64, 4778,   26,  753,   48,  668,   69, 2696,  531,\n",
      "         553,   35, 1843,  417,  131,   57,  187,   26,   12,   26,   27,  979,\n",
      "          35, 1656,   40,   39,  160,  280,   42,   32,   39,   26,  823,  213,\n",
      "          26, 3687, 2833, 1486,   13, 2966,  930, 4273,   60,   26, 4135, 8547,\n",
      "          35,   35,  314,  310,  310, 4816, 2058, 6019, 6384,  718,  416, 9054,\n",
      "          14,  467, 1652, 3817,   42, 3320, 4897, 3672,  290,  852,   48, 9723,\n",
      "          69, 3126,   24,   93,   24,  220,   27,   24,   15,   35,   24,  774,\n",
      "        1522,   42,  152,  108,   26,  181, 1557,  383,   35,   64,   78,   32,\n",
      "        3220, 3695,  468,  563])\n"
     ]
    }
   ],
   "source": [
    "# test the get batch function\n",
    "ts_data, ts_target = get_batch(data_source=train_data, seq_length=5, start_index=10)\n",
    "print(ts_data, \"\\n\\n\", ts_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c812f8a0-a529-45a2-9d51-2e7f557a8b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c3f4b-b890-447d-ad89-624b831f1f93",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create a function to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e8d78d2-8abe-44ef-b5a0-139e7468edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a evaluation function to evaluate the model\n",
    "def evaluate_rnn(model, criterion, data_source, batch_size, seq_length):\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    hidden = model.init_hidden(batch_size=batch_size)\n",
    "    \n",
    "    # perform evaluation\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, seq_length):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            total_loss += len(data) * criterion(output, targets).item()\n",
    "    \n",
    "    # return average loss\n",
    "    return total_loss / (len(data_source)-1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade5b28-6daf-4a35-95ed-e6e45d0a5560",
   "metadata": {},
   "source": [
    "#### Create a function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9272865a-cc70-4bd1-940f-52ed0a83300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training function to train the model\n",
    "def train_rnn(model, training_data, seq_length, criterion, batch_size, epochs):\n",
    "    # put the model in train mode\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    hidden = model.init_hidden(batch_size=batch_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch, i in enumerate(range(0, training_data.size(0)-1, seq_length)): #(sequence_length*batch_size))):\n",
    "            data, targets = get_batch(training_data, seq_length, i)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            # perform forward pass through the model\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output, hidden = model(data, hidden)\n",
    "            # compute loss for current batch\n",
    "            # and backpopagate gradients\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "            #for p in model.parameters():\n",
    "            #    p.data.add_(p.grad, alpha=-lr)\n",
    "            # update total loss\n",
    "            total_loss += loss.item()\n",
    "            # print batch statistics\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "249ea9ef-b4d3-4a80-992e-9391e77a61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instanc eof the model\n",
    "model = simpleRNN(num_tokens=len(word_index_dict), embedding_dim=200, hidden_dim=200, num_layers=2)\n",
    "#(self, num_embeddings, embedding_dim, hidden_dim, n_layers, output_size, dropout=0.5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd154f37-b8f5-48c4-9c4e-4b62b54f2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rnn(model, train_data, sequence_length, criterion, batch_size, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab70806-08fa-4eec-828e-27cc18907f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
